<body class="typora-export">
# 预处理

1. 预处理的一些通用方法：

    * `get_params([deep])`：返回模型的参数。

        * `deep`： 如果为`True`，则可以返回模型参数的子对象。

    * `set_params(**params)`：设置模型的参数。

        * `params`：待设置的关键字参数。

    * `fit(X[, y])` ：获取预处理需要的参数（如：特征的最大值、最小值等），不同的预处理方法需要的参数不同。

        * `X` ：训练集样本集合。通常是一个`numpy array`，每行代表一个样本，每列代表一个特征。
        * `y` ：训练样本的标签集合。它与`X` 的每一行相对应。

    * `transform(X[, copy])`：执行预处理，返回处理后的样本集。

        * `X` ：训练集样本集合。通常是一个`numpy array`，每行代表一个样本，每列代表一个特征。
        * `copy` ：一个布尔值，指定是否拷贝数据。

    * `fit_transform(X[, y])` ：获取预处理需要的参数并执行预处理，返回处理后的样本集。

        * `X` ：训练集样本集合。通常是一个`numpy array`，每行代表一个样本，每列代表一个特征。
        * `y` ：训练样本的标签集合。它与`X` 的每一行相对应。


2. 预处理的一些通用参数：

    * `copy`： 一个布尔值，指定是否拷贝数据。

    如果为`False`则执行原地修改。此时节省空间，但修改了原始数据。



## 一、特征处理

### 1.1 二元化

1. 二元化`Binarizer` 的原型为：

```
class sklearn.preprocessing.Binarizer(threshold=0.0, copy=True)
```

    * `threshold`：一个浮点数，它指定了转换阈值：低于此阈值的值转换为0，高于此阈值的值转换为 1。
    * `copy`：一个布尔值，指定是否拷贝数据。

2. 方法：<br></br>

    * `fit(X[, y])` ：不作任何事情，主要用于为流水线`Pipeline` 提供接口。
    * `transform(X[, copy])` ：将每个样本的特征二元化。
    * `fit_transform(X[, y])` ：将每个样本的特征二元化。


### 1.2 独热码

1. 独热码`OneHotEncoder` 的原型为：

```
xxxxxxxxxxclass sklearn.preprocessing.OneHotEncoder(n_values='auto', categorical_features='all',dtype=<class 'float'>, sparse=True, handle_unknown='error')
```

    * `n_values`：字符串`'auto'`，或者一个整数，或者一个整数的数组，它指定了样本每个特征取值的上界（特征的取值为从0开始的整数）：

        * `'auto'`：自动从训练数据中推断特征值取值的上界。
        * 一个整数：指定了所有特征取值的上界。
        * 一个整数的数组：每个元素依次指定了每个特征取值的上界。

    * `categorical_features` ：字符串`'all'`，或者下标的数组，或者是一个`mask`，指定哪些特征需要独热码编码 ：

        * `'all'`：所有的特征都将独热码编码。
        * 一个下标的数组：指定下标的特征将独热码编码。
        * 一个`mask`：对应为`True`的特征将编码为独热码。

    所有的非`categorical` 特征都将被安排在`categorical` 特征的右边。

    * `dtype`：一个类型，指定了独热码编码的数值类型，默认为`np.float` 。

    * `sparse`：一个布尔值，指定编码结果是否作为稀疏矩阵。

    * `handle_unknown`：一个字符串，指定转换过程中遇到了未知的 `categorical` 特征时的异常处理策略。可以为：

        * `'error'`：抛出异常。
        * `'ignore'`：忽略。


2. 属性：

    * `active_features_`：一个索引数组，存放转换后的特征中哪些是由独热码编码而来。

    仅当`n_values='auto'`时该属性有效。

    * `feature_indices_`：一个索引数组，存放原始特征和转换后特征位置的映射关系。

    第 `i` 个原始特征将被映射到转换后的`[feature_indices_[i],feature_indices_[i+1])` 之间的特征。

    * `n_values_`：一个计数数组，存放每个原始特征取值的种类。

    一般为训练数据中该特征取值的最大值加1，这是因为默认每个特征取值从零开始。


3. 方法：

    * `fit(X[, y])` ：训练编码器。
    * `transform(X)` ：执行独热码编码。
    * `fit_transform(X[, y])` ：训练编码器，然后执行独热码编码。


### 1.3 标准化

#### 1.3.1 MinMaxScaler

1. `MinMaxScaler`实现了`min-max`标准化，其原型为：

```
xxxxxxxxxxclass sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)
```

    * `feature_range`：一个元组`(min,max)`，指定了执行变换之后特征的取值范围。
    * `copy`：一个布尔值，指定是否拷贝数据。

2. 属性：

    * `min_`：一个数组，给出了每个特征的原始最小值的调整值。

    设特征<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="2.461ex" role="img" style="vertical-align: -0.705ex; margin-left: -0.028ex;" viewbox="-12 -755.9 424 1059.4" width="0.985ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" id="E5-MJMATHI-6A" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E5-MJMATHI-6A" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></svg></span><script id="MathJax-Element-4" type="math/tex">j</script>的原始最小值为<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="2.461ex" role="img" style="vertical-align: -0.705ex; margin-left: -0.028ex;" viewbox="-12 -755.9 1702.7 1059.4" width="3.955ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" id="E3-MJMATHI-6A" stroke-width="0"></path><path d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" id="E3-MJMAIN-6D" stroke-width="0"></path><path d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" id="E3-MJMAIN-69" stroke-width="0"></path><path d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" id="E3-MJMAIN-6E" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E3-MJMATHI-6A" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><g transform="translate(412,-150)"><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-6D" xmlns:xlink="http://www.w3.org/1999/xlink"></use><use transform="scale(0.707)" x="833" xlink:href="#E3-MJMAIN-69" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.707)" x="1111" xlink:href="#E3-MJMAIN-6E" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></g></svg></span><script id="MathJax-Element-2" type="math/tex">j_{\min}</script>，原始最大值为<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="2.461ex" role="img" style="vertical-align: -0.705ex; margin-left: -0.028ex;" viewbox="-12 -755.9 1839.9 1059.4" width="4.273ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" id="E4-MJMATHI-6A" stroke-width="0"></path><path d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" id="E4-MJMAIN-6D" stroke-width="0"></path><path d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" id="E4-MJMAIN-61" stroke-width="0"></path><path d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" id="E4-MJMAIN-78" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E4-MJMATHI-6A" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><g transform="translate(412,-150)"><use transform="scale(0.707)" xlink:href="#E4-MJMAIN-6D" xmlns:xlink="http://www.w3.org/1999/xlink"></use><use transform="scale(0.707)" x="833" xlink:href="#E4-MJMAIN-61" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.707)" x="1333" xlink:href="#E4-MJMAIN-78" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></g></svg></span><script id="MathJax-Element-3" type="math/tex">j_{\max}</script>。则特征<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="2.461ex" role="img" style="vertical-align: -0.705ex; margin-left: -0.028ex;" viewbox="-12 -755.9 424 1059.4" width="0.985ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" id="E5-MJMATHI-6A" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E5-MJMATHI-6A" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></svg></span><script id="MathJax-Element-4" type="math/tex">j</script>的原始最小值的调整值为：<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="3.978ex" role="img" style="vertical-align: -1.405ex;" viewbox="0 -1107.7 3398.2 1712.8" width="7.893ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" id="E6-MJMATHI-6A" stroke-width="0"></path><path d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" id="E6-MJMAIN-6D" stroke-width="0"></path><path d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" id="E6-MJMAIN-69" stroke-width="0"></path><path d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" id="E6-MJMAIN-6E" stroke-width="0"></path><path d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" id="E6-MJMAIN-61" stroke-width="0"></path><path d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" id="E6-MJMAIN-78" stroke-width="0"></path><path d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" id="E6-MJMAIN-2212" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(120,0)"><rect height="60" stroke="none" width="3158" x="0" y="220"></rect><g transform="translate(981,573)"><use transform="scale(0.707)" x="0" xlink:href="#E6-MJMATHI-6A" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><g transform="translate(291,-170)"><use transform="scale(0.5)" xlink:href="#E6-MJMAIN-6D" xmlns:xlink="http://www.w3.org/1999/xlink"></use><use transform="scale(0.5)" x="833" xlink:href="#E6-MJMAIN-69" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.5)" x="1111" xlink:href="#E6-MJMAIN-6E" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></g><g transform="translate(60,-372)"><use transform="scale(0.707)" x="0" xlink:href="#E6-MJMATHI-6A" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><g transform="translate(291,-170)"><use transform="scale(0.5)" xlink:href="#E6-MJMAIN-6D" xmlns:xlink="http://www.w3.org/1999/xlink"></use><use transform="scale(0.5)" x="833" xlink:href="#E6-MJMAIN-61" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.5)" x="1333" xlink:href="#E6-MJMAIN-78" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g><use transform="scale(0.707)" x="1827" xlink:href="#E6-MJMAIN-2212" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><g transform="translate(1842,0)"><use transform="scale(0.707)" x="0" xlink:href="#E6-MJMATHI-6A" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><g transform="translate(291,-170)"><use transform="scale(0.5)" xlink:href="#E6-MJMAIN-6D" xmlns:xlink="http://www.w3.org/1999/xlink"></use><use transform="scale(0.5)" x="833" xlink:href="#E6-MJMAIN-69" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.5)" x="1111" xlink:href="#E6-MJMAIN-6E" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></g></g></g></g></svg></span><script id="MathJax-Element-5" type="math/tex">\frac{j_{\min}}{j_{\max}-j_{\min}}</script>。

    * `scale_`：一个数组，给出了每个特征的缩放倍数 。

    * `data_min_`：一个数组，给出了每个特征的原始最小值 。

    * `data_max_`：一个数组，给出了每个特征的原始最大值。

    * `data_range_`：一个数组，给出了每个特征的原始的范围（最大值减最小值）。


3. 方法：<br></br>

    * `fit(X[, y])` ：计算每个特征的最小值和最大值，从而为后续的转换做准备。

    * `transform(X)` ：执行特征的标准化。

    * `fit_transform(X[, y])` ：计算每个特征的最大小值和最大值，然后执行特征的标准化。

    * `inverse_transform(X)`：逆标准化，还原成原始数据。

    * `partial_fit(X[, y])` ：学习部分数据，计算每个特征的最小值和最大值，从而为后续的转换做准备。

    它支持批量学习，这样对于内存更友好。即训练数据并不是一次性学习，而是分批学习。



#### 1.3.2 MaxAbsScaler

1. `MaxAbsScaler` 实现了`max-abs` 标准化，其原型为：

```
xxxxxxxxxxclass sklearn.preprocessing.MaxAbsScaler(copy=True)
```

    * `copy`：一个布尔值，指定是否拷贝数据。

2. 属性：<br></br>

    * `scale_`：一个数组，给出了每个特征的缩放倍数的倒数。
    * `max_abs_`：一个数组，给出了每个特征的绝对值的最大值。
    * `n_samples_seen_`：一个整数，给出了当前已经处理的样本的数量（用于分批训练）。

3. 方法：参考`MinMaxScaler` 。


#### 1.3.3 StandardScaler

1. `StandardScaler`实现了 `z-score`标准化，其原型为：

```
xxxxxxxxxxclass sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)
```

    * `copy`：一个布尔值，指定是否拷贝数据。

    * `with_mean`：一个布尔值，指定是否中心化。

        * 如果为`True`，则缩放之前先将每个特征中心化（即特征值减去该特征的均值）。
        * 如果元素数据是稀疏矩阵的形式，则不能指定`with_mean=True` 。

    * `with_std`：一个布尔值，指定是否方差归一化。

    如果为`True`，则缩放每个特征到单位方差。


2. 属性：<br></br>

    * `scale_`：一个数组，给出了每个特征的缩放倍数的倒数。
    * `mean_`：一个数组，给出了原始数据每个特征的均值。
    * `var_`：一个数组，给出了原始数据每个特征的方差。
    * `n_samples_seen_`：一个整数，给出了当前已经处理的样本的数量（用于分批训练）。

3. 方法：参考`MinMaxScaler` 。


### 1.4 正则化

1. `Normalizer` 实现了数据正则化，其原型为：

```
xxxxxxxxxxclass sklearn.preprocessing.Normalizer(norm='l2', copy=True)
```

    * `norm`：一个字符串，指定正则化方法。可以为：

        * `'l1'`：采用<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-14-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="2.227ex" role="img" style="vertical-align: -0.472ex;" viewbox="0 -755.9 1134.6 958.9" width="2.635ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" id="E15-MJMATHI-4C" stroke-width="0"></path><path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" id="E15-MJMAIN-31" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E15-MJMATHI-4C" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.707)" x="963" xlink:href="#E15-MJMAIN-31" xmlns:xlink="http://www.w3.org/1999/xlink" y="-213"></use></g></svg></span><script id="MathJax-Element-14" type="math/tex">L_1</script>范数正则化。
        * `'l2'`：采用<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="2.227ex" role="img" style="vertical-align: -0.472ex;" viewbox="0 -755.9 1134.6 958.9" width="2.635ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" id="E8-MJMATHI-4C" stroke-width="0"></path><path d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" id="E8-MJMAIN-32" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E8-MJMATHI-4C" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.707)" x="963" xlink:href="#E8-MJMAIN-32" xmlns:xlink="http://www.w3.org/1999/xlink" y="-213"></use></g></svg></span><script id="MathJax-Element-7" type="math/tex">L_2</script>范数正则化。
        * `'max'`：采用<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="2.344ex" role="img" style="vertical-align: -0.588ex;" viewbox="0 -755.9 1488.1 1009.2" width="3.456ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" id="E9-MJMATHI-4C" stroke-width="0"></path><path d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z" id="E9-MJMAIN-221E" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E9-MJMATHI-4C" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.707)" x="963" xlink:href="#E9-MJMAIN-221E" xmlns:xlink="http://www.w3.org/1999/xlink" y="-213"></use></g></svg></span><script id="MathJax-Element-8" type="math/tex">L_\infty</script>范数正则化。

    * `copy`：一个布尔值，指定是否拷贝数据。


2. 方法：<br></br>

    * `fit(X[, y])` ：不作任何事情，主要用于为流水线`Pipeline` 提供接口。
    * `transform(X[, y, copy])` ：将每一个样本正则化为范数等于单位1。
    * `fit_transform(X[, y])` ：将每一个样本正则化为范数等于单位1。


## 二、特征选择

### 2.1 过滤式特征选取

#### 2.1.1 VarianceThreshold

1. `VarianceThreshold` 用于剔除方差很小的特征，其原型为：

```
xxxxxxxxxxclass sklearn.feature_selection.VarianceThreshold(threshold=0.0)
```

    * `threshold`：一个浮点数，指定方差的阈值。低于此阈值的特征将被剔除。

2. 属性：<br></br>

    * `variances_`：一个数组，元素分别是各特征的方差。

3. 方法：

    * `fit(X[, y])`：从样本数据中学习每个特征的方差。

    * `transform(X)`：执行特征选择，即删除低于指定阈值的特征。

    * `fit_transform(X[, y])`：从样本数据中学习每个特征的方差，然后执行特征选择。

    * `get_support([indices])`：返回保留的特征。

        * 如果`indices=True`，则返回被选出的特征的索引。
        * 如果`indices=False`，则返回一个布尔值组成的数组，该数组指示哪些特征被选择。

    * `inverse_transform(X)`：根据被选出来的特征还原原始数据（特征选取的逆操作），但是对于被删除的特征的值全部用 0 代替。



#### 2.1.2 SelectKBest

1. `SelectKBest` 用于保留统计得分最高的<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-13-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="1.994ex" role="img" style="vertical-align: -0.238ex;" viewbox="0 -755.9 521 858.4" width="1.21ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" id="E14-MJMATHI-6B" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E14-MJMATHI-6B" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></svg></span><script id="MathJax-Element-13" type="math/tex">k</script>个特征，其原型为：

```
xxxxxxxxxxclass sklearn.feature_selection.SelectKBest(score_func=<function f_classif>, k=10)
```

    * `score_func`：一个函数，用于给出统计指标。

    该函数的参数为 `(X,y)` ，返回值为`(scores,pvalues)` 。

        * `X` ：样本集合。通常是一个`numpy array`，每行代表一个样本，每列代表一个特征。
        * `y` ：样本的标签集合。它与`X` 的每一行相对应。
        * `scores` ：样本的得分集合。它与`X` 的每一行相对应。
        * `pvalues`：样本得分的`p` 值。它与`X` 的每一行相对应。

    * `k`：一个整数或者字符串`'all'`，指定要保留最佳的几个特征。

    如果为`'all'`，则保留所有的特征。


2. `sklearn`提供的常用的统计指标函数为：

    * `sklearn.feature_selection.f_regression`：基于线性回归分析来计算统计指标，适用于回归问题。
    * `sklearn.feature_selection.chi2`：计算卡方统计量，适用于分类问题。
    * `sklearn.feature_selection.f_classif`：根据方差分析`Analysis of variance：ANOVA`的原理，依靠`F-分布`为机率分布的依据，利用平方和与自由度所计算的组间与组内均方估计出`F`值。适用于分类问题 。

3. 属性：<br></br>

    * `scores_`：一个数组，给出了所有特征的得分。
    * `pvalues_`：一个数组，给出了所有特征得分的`p-values` 。

4. 方法：参考`VarianceThreshold` 。


#### 2.1.3 SelectPercentile

1. `SelectPercentile` 用于保留统计得分最高的<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-10-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="2.227ex" role="img" style="vertical-align: -0.355ex;" viewbox="0 -806.1 1354 958.9" width="3.145ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" id="E11-MJMATHI-6B" stroke-width="0"></path><path d="M465 605Q428 605 394 614T340 632T319 641Q332 608 332 548Q332 458 293 403T202 347Q145 347 101 402T56 548Q56 637 101 693T202 750Q241 750 272 719Q359 642 464 642Q580 642 650 732Q662 748 668 749Q670 750 673 750Q682 750 688 743T693 726Q178 -47 170 -52Q166 -56 160 -56Q147 -56 142 -45Q137 -36 142 -27Q143 -24 363 304Q469 462 525 546T581 630Q528 605 465 605ZM207 385Q235 385 263 427T292 548Q292 617 267 664T200 712Q193 712 186 709T167 698T147 668T134 615Q132 595 132 548V527Q132 436 165 403Q183 385 203 385H207ZM500 146Q500 234 544 290T647 347Q699 347 737 292T776 146T737 0T646 -56Q590 -56 545 0T500 146ZM651 -18Q679 -18 707 24T736 146Q736 215 711 262T644 309Q637 309 630 306T611 295T591 265T578 212Q577 200 577 146V124Q577 -18 647 -18H651Z" id="E11-MJMAIN-25" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E11-MJMATHI-6B" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use x="521" xlink:href="#E11-MJMAIN-25" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></svg></span><script id="MathJax-Element-10" type="math/tex">k\%</script>比例的特征，其原型为：

```
xxxxxxxxxxclass sklearn.feature_selection.SelectPercentile(score_func=<function f_classif>,percentile=10)
```

    * `score_func`：一个函数，用于给出统计指标。参考`SelectKBest` 。
    * `percentile`：一个整数，指定要保留最佳的百分之几的特征，如`10`表示保留最佳的百分之十的特征

2. 属性：参考`SelectKBest` 。

3. 方法：参考`VarianceThreshold` 。


### 2.2 包裹式特征选取

#### 2.2.1 RFE

1. `RFE`类用于实现包裹式特征选取，其原型为：

```
xxxxxxxxxxclass sklearn.feature_selection.RFE(estimator, n_features_to_select=None,step=1,verbose=0)
```

    * `estimator`：一个学习器，它必须提供一个`.fit`方法和一个`.coef_`特征。其中`.coef_`特征中存放的是学习到的各特征的权重系数。

    通常使用`SVM`和广义线性模型作为`estimator`参数。

    * `n_features_to_select`：一个整数或者`None`，指定要选出几个特征。

    如果为`None`，则默认选取一半的特征。

    * `step`：一个整数或者浮点数，指定每次迭代要剔除权重最小的几个特征。

        * 如果大于等于1，则作为整数，指定每次迭代要剔除特征的数量。
        * 如果在`0.0~1.0`之间，则指定每次迭代要剔除特征的比例。

    * `verbose`：一个整数，控制输出日志。


2. `RFE`要求学习器能够学习特征的权重（如线性模型），其原理为：<br></br>

    * 首先学习器在初始的特征集合上训练。
    * 然后学习器学得每个特征的权重，剔除当前权重一批特征，构成新的训练集。
    * 再将学习器在新的训练集上训练，直到剩下的特征的数量满足条件。

3. 属性：

    * `n_features_`：一个整数，给出了被选出的特征的数量。
    * `support_`：一个数组，给出了特征是否被选择的`mask` 。
    * `ranking_`：特征权重排名。原始第 `i` 个特征的排名为 `raning_[i]` 。
    * `estimator_`： 外部提供的学习器 。

4. 方法：<br></br>

    * `fit(X,y)`：训练`RFE`模型

    * `transform(X)`：执行特征选择。

    * `fit_transform(X,y)`：从样本数据中学习`RFE`模型，然后执行特征选择。

    * `get_support([indices])`：返回保留的特征。

        * 如果`indices=True`，则返回被选出的特征的索引。
        * 如果`indices=False`，则返回一个布尔值组成的数组，该数组指示哪些特征被选择。

    * `inverse_transform(X)`：根据被选出来的特征还原原始数据（特征选取的逆操作），但是对于被删除的特征值全部用 0 代替。

    * `predict(X)/predict_log_proba(X) /predict_proba(X)`：将`X`进行特征选择之后，在使用内部的`estimator`来预测。

    * `score(X, y)` ：将`X`进行特征选择之后，训练内部`estimator` 并对内部的`estimator`进行评分。



#### 2.2.2 RFECV

1. `RFECV`是`RFE`的一个变体，它执行一个交叉验证来寻找最优的剩余特征数量，因此不需要指定保留多少个特征。

2. `RFECV` 的原型为：

```
xxxxxxxxxxclass sklearn.feature_selection.RFECV(estimator, step=1, cv=None, scoring=None,verbose=0)
```

    * `cv`：一个整数，或者交叉验证生成器或者一个可迭代对象，它决定了交叉验证策略。

        * 如果为`None`，则使用默认的`3`折交叉验证。
        * 如果为整数<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-13-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="1.994ex" role="img" style="vertical-align: -0.238ex;" viewbox="0 -755.9 521 858.4" width="1.21ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" id="E14-MJMATHI-6B" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E14-MJMATHI-6B" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></svg></span><script id="MathJax-Element-13" type="math/tex">k</script>，则使用<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-13-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="1.994ex" role="img" style="vertical-align: -0.238ex;" viewbox="0 -755.9 521 858.4" width="1.21ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" id="E14-MJMATHI-6B" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E14-MJMATHI-6B" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></svg></span><script id="MathJax-Element-13" type="math/tex">k</script>折交叉验证。
        * 如果为交叉验证生成器，则直接使用该对象。
        * 如果为可迭代对象，则使用该可迭代对象迭代生成`训练-测试`集合。

    * 其它参数参考`RFE` 。


3. 属性：<br></br>

    * `grid_scores_`：一个数组，给出了交叉验证的预测性能得分。其元素为每个特征子集上执行交叉验证后的预测得分。
    * 其它属性参考`RFE` 。

4. 方法：参考`RFE` 。


### 2.3 嵌入式特征选择

1. `SelectFromModel`用于实现嵌入式特征选取，其原型为：

```
xxxxxxxxxxclass sklearn.feature_selection.SelectFromModel(estimator, threshold=None, prefit=False)
```

    * `estimator`：一个学习器，它可以是未训练的(`prefit=False`)，或者是已经训练好的(`prefit=True`)。

    `estimator` 必须有`coef_`或者`feature_importances_`属性，给出每个特征的重要性。当某个特征的重要性低于某个阈值时，该特征将被移除。

    * `threshold`：一个字符串或者浮点数或者`None`，指定特征重要性的一个阈值。低于此阈值的特征将被剔除。

        * 如果为浮点数，则指定阈值的绝对大小。

        * 如果为字符串，可以是：

            * `'mean'`：阈值为特征重要性的均值。
            * `'median'`：阈值为特征重要性的中值。
            * 如果是`'1.5*mean'`，则表示阈值为 1.5 倍的特征重要性的均值。

        * 如果为`None`：

            * 如果`estimator`有一个`penalty`参数，且该参数设置为`'l1'`，则阈值默认为`1e-5`。
            * 其他情况下，阈值默认为`'mean'` 。


    * `prefit`：一个布尔值，指定`estimator`是否已经训练好了。

    如果`prefit=False`，则`estimator`是未训练的。


2. 属性：<br></br>

    * `threshold_`：一个浮点数，存储了用于特征选取重要性的阈值。

3. 方法：<br></br>

    * `fit(X,y)`：训练`SelectFromModel`模型。

    * `transform(X)`：执行特征选择。

    * `fit_transform(X,y)`：从样本数据中学习`SelectFromModel`模型，然后执行特征选择。

    * `get_support([indices])`：返回保留的特征。

        * 如果`indices=True`，则返回被选出的特征的索引。
        * 如果`indices=False`，则返回一个布尔值组成的数组，该数组指示哪些特征被选择。

    * `inverse_transform(X)`：根据被选出来的特征还原原始数据（特征选取的逆操作），但是对于被删除的特征值全部用 0 代替。

    * `partial_fit(X[, y])`：通过部分数据来学习`SelectFromModel`模型。

    它支持批量学习，这样对于内存更友好。即训练数据并不是一次性学习，而是分批学习。



## 三、字典学习

### 3.1 DictionaryLearning

1. `DictionaryLearning`用于字典学习，其原型为：

```
xxxxxxxxxxclass sklearn.decomposition.DictionaryLearning(n_components=None, alpha=1,max_iter=1000, tol=1e-08, fit_algorithm='lars', transform_algorithm='omp',transform_n_nonzero_coefs=None, transform_alpha=None, n_jobs=1,code_init=None, dict_init=None, verbose=False, split_sign=False, random_state=None)
```

    * `n_components`：一个整数，指定了字典大小<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-13-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="1.994ex" role="img" style="vertical-align: -0.238ex;" viewbox="0 -755.9 521 858.4" width="1.21ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" id="E14-MJMATHI-6B" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E14-MJMATHI-6B" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></svg></span><script id="MathJax-Element-13" type="math/tex">k</script>。

    * `alpha`：一个浮点数，指定了<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-14-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="2.227ex" role="img" style="vertical-align: -0.472ex;" viewbox="0 -755.9 1134.6 958.9" width="2.635ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" id="E15-MJMATHI-4C" stroke-width="0"></path><path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" id="E15-MJMAIN-31" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E15-MJMATHI-4C" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.707)" x="963" xlink:href="#E15-MJMAIN-31" xmlns:xlink="http://www.w3.org/1999/xlink" y="-213"></use></g></svg></span><script id="MathJax-Element-14" type="math/tex">L_1</script>正则化项的系数<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-15-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="1.994ex" role="img" style="vertical-align: -0.238ex;" viewbox="0 -755.9 583 858.4" width="1.354ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z" id="E16-MJMATHI-3BB" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E16-MJMATHI-3BB" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></svg></span><script id="MathJax-Element-15" type="math/tex">\lambda</script>，它控制了稀疏性。

    * `max_iter`：一个整数，指定了最大迭代次数。

    * `tol`：一个浮点数，指定了收敛阈值。

    * `fit_algorithm`：一个字符串，指定了求解算法。可以为：

        * `'lars'`：使用`least angle regression`算法来求解。
        * `'cd'`：使用`coordinate descent`算法来求解。

    * `transform_algorithm`：一个字符串，指定了数据转换的方法。可以为：

        * `'lasso_lars'`：使用`Lars`算法来求解。
        * `'lasso_cd'`：使用`coordinate descent`算法来求解。
        * `'lars'`：使用`least angle regression`算法来求解。
        * `'omp'`：使用正交匹配的方法来求解。
        * `'threshold'`：通过字典转换后的坐标中，小于 `transform_alpha` 的特征的值都设成零。

    * `transform_n_nonzero_coefs`：一个整数，指定解中每一列中非零元素的个数，默认为`0.1*n_features`。

    只用于`lars`算法和`omp`算法（`omp`算法中，可能被`transform_alpha`参数覆盖）。

    * `transform_alpha`：一个浮点数，默认为 1.0 。

        * 如果算法为`lasso_lars`或者`lasso_cd`，则该参数指定了`L1`正则化项的系数。
        * 如果算法为`threshold`，则该参数指定了特征为零的阈值。
        * 如果算法为`omp`，则该参数指定了重构误差的阈值，此时它覆盖了`transform_n_nonzero_coefs`参数。

    * `split_sign`：一个布尔值，指定是否拆分系数特征向量为其正向值和负向值的拼接。

    * `n_jobs`：一个整数，指定并行性。

    * `code_init`：一个数组，指定了初始编码，它用于字典学习算法的热启动。

    * `dict_init`：一个数组，指定了初始字典，它用于字典学习算法的热启动。

    * `verbose`：一个整数，控制输出日志。

    * `random_state` ：一个整数或者一个`RandomState` 实例，或者`None`，指定随机数种子。


2. 属性：<br></br>

    * `components_`：一个数组，存放学到的字典。
    * `error_`：一个数组，存放每一轮迭代的误差。
    * `n_iter_`：一个整数，存放迭代的次数。

3. 方法：<br></br>

    * `fit(X,y)`：学习字典。
    * `transform(X)`：根据学到的字典进行编码。
    * `fit_transform(X,y)`：学习字典并执行字典编码。


### 3.2 MiniBatchDictionaryLearning

1. `MiniBatchDictionaryLearning`也是字典学习，它主要用于大规模数据。它每次训练一批样本，然后连续多次训练。

2. `MiniBatchDictionaryLearning` 的原型为：

```
xxxxxxxxxxclass sklearn.decomposition.MiniBatchDictionaryLearning(n_components=None, alpha=1,n_iter=1000, fit_algorithm='lars', n_jobs=1, batch_size=3, shuffle=True,dict_init=None, transform_algorithm='omp', transform_n_nonzero_coefs=None,transform_alpha=None, verbose=False, split_sign=False, random_state=None)
```

    * `n_iter`：一个整数，指定了总的执行迭代的数量。
    * `batch_size`：一个整数，指定了每次训练时的样本数量。
    * `shuffle`：一个布尔值，指定在训练每一批样本之前，是否对该批次样本进行混洗。
    * 其它参数参考`DictionaryLearning` 。

3. 属性：<br></br>

    * `components_`：一个数组，存放学到的字典。
    * `inner_stats_`：数组的元组，存放算法的中间状态。
    * `n_iter_`：一个整数，存放迭代的次数。

4. 方法：<br></br>

    * `fit(X,y)`：学习字典。
    * `transform(X)`：根据学到的字典进行编码。
    * `fit_transform(X,y)`：学习字典并执行字典编码。
    * `partial_fit(X[, y, iter_offset])`：只训练一个批次的样本。


## 四、PipeLine

1. `scikit-learn` 中的流水线的流程通常为：

    * 通过一组特征处理`estimator` 来对特征进行处理（如标准化、正则化）。
    * 通过一组特征提取`estimator`来提取特征。
    * 通过一个模型预测 `estimator` 来学习模型，并执行预测。

除了最后一个 `estimator` 之外，前面的所有的 `estimator` 必须提供`transform`方法。该方法用于执行数据变换（如归一化、正则化、以及特征提取等）。

2. `Pipeline`将多个`estimator`组成流水线，其原型为：

```
xxxxxxxxxxclass sklearn.pipeline.Pipeline(steps)
```

    * `steps`：一个列表，列表的元素为`(name,transform)`元组。其中：

        * `name`是 `estimator` 的名字，用于输出和日志
        * `transform`是 `estimator` 。之所以叫`transform`是因为这个 `estimator` （除了最后一个）必须提供`transform`方法。


3. 属性：

    * `named_steps`：一个字典。键就是`steps`中各元组的`name`元素，字典的值就是`steps`中各元组的`transform`元素。

4. 方法：

    * `fit(X[, y])`：启动流水线，依次对各个`estimator`（除了最后一个）执行`.fit`方法和`.transform`方法转换数据；对最后一个`estimator`执行`.fit`方法训练学习器。

    * `transform(X)`：启动流水线，依次对各个`estimator` （包括最后一个）执行`.fit`方法和`.transform`方法转换数据。

    * `fit_transform(X[, y])`：启动流水线，依次对各个`estimator`（除了最后一个）执行`.fit`方法和`.transform`方法转换数据；对最后一个`estimator`执行`.fit_transform`方法转换数据。<br></br>

    * `inverse_transform(X)`：将转换后的数据逆转换成原始数据。

    要求每个`estimator`都实现了`.inverse_transform`方法。

    * `predict(X)/predict_log_proba(X) /predict_proba(X)`：将`X`进行数据转换后，用最后一个学习器来预测。

    * `score(X, y)` ：将`X`进行数据转换后，训练最后一个`estimator` ，并对最后一个`estimator` 评分。


</body>