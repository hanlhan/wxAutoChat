<body class="typora-export">
# 简介

## 一、 介绍

1. 深度学习：计算机从经验中学习，以层次化的概念（`concept`）来理解世界。

    * 从经验中学习：避免了人工指定计算机学习所需的所有知识。
    * 层次化的概念：计算机通过从简单的概念来构建、学习更复杂的概念。

如果绘制一张图来展示这些概念的关系，那么这张图是一个深度的层次结构，因此称这种方法为深度学习。


### 1.1 知识表达

1. 计算机需要获取大量的`"常识"` 才能以人工智能的方式行动，如：`树叶是绿色的、乒乓球比足球小`。

这些`"常识"`大部分是主观的和直观的，因此很难以正式的方式表达。一个问题是：如何将这些`"常识"` 传给计算机。

有两种方式将知识传递给计算机：

    * 知识库（`knowledge base`）：通过形式化语言硬编码关于真实世界的知识，计算机使用逻辑推理自动推理这些硬编码的知识。

    最出名的知识库项目是`Cyc`，但这些项目都没有取得重大成功。

    * 机器学习：`AI` 系统通过从原始数据中提取模式来获得知识，并作出看起来`"智能"`的决策。

    如：通过朴素贝叶斯算法分离正常的电子邮件和垃圾邮件。


2. 传统机器学习算法严重依赖于数据的表达方式（`representation`）。如：对病人的诊断中，`AI`系统并不是直接接触病人，而是由医生告诉`AI`系统关于病人的一些信息（如身高、体重等）。这些信息称作特征（`feature`）。

传统机器学习算法在三个层面上严重依赖于数据的表达方式：

    * 传统机器学习算法无法确定需要哪些特征。如：是直接给身高和体重，还是给出肥胖系数？
    * 传统机器学习算法也无法确定这些特征的方式。如：是否将特征离散化？
    * 某些信息，传统的机器学习算法无法学习。如：给出一份核磁共振的影像，由于每一个像素点与诊断结果相关性非常微小，因此传统机器学习算法无法学习。

3. 这种对数据的表达方式的依赖是计算机科学甚至生活中的一般现象。

    * 如：生活中人们很容易对阿拉伯数字进行算术运算，但是对于罗马数字的算术运算更费时间。

    * 下图的线性分类任务中：左图采用笛卡尔坐标系，右图采用极坐标系。

    可以看到数据的不同表达方式（坐标系的不同）导致左图难以线性分类，右图可以容易的线性分类。

    ![](../imgs/dl_intro/representation.png)


4. 在传统的机器学习应用中，通常针对特定的任务来设计一套专用的、有效的特征集合，然后采集这些特征描述下的数据。如：语音识别中， 一个有效的特征就是讲话者的声音的声道（`vocal tract`）。

但是大多数任务中，很难给出有效的特征是哪些。如：从图片中检查汽车的任务，可以使用是否有轮子作为一个特征。但是很难根据像素点来准确描述轮子。因为可能由于阴影、光照条件、观察角度等导致轮子的像素集合非常复杂。

5. 特征设计的一个解决方案是：通过机器学习来发现特征。即：**不仅学习`representation`到输出的映射（即模型），也学习`representation`本身** 。这称作表达学习（`reprensentation learning`）。

其优点有：

    * 往往比人为设计的特征的性能要好得多。

    * 允许`AI`系统快速适应新任务，用最少的人工干预。

    * 对于简单任务它可以在几分钟内学到一组好的特征，对于复杂任务可以在几小时到几个月的时间内学到一组好的特征。

    在复杂任务中，人工设计特征需要消耗大量的人力和时间。



### 1.2 特征的组合

1. 在设计特征或者学习特征时，一个良好的准则是：将能够解释数据的那些变化因子分离。

通常这些因子不是直接观察到的量，而是影响那些能够直接观察到的量。如：在语音识别中，变化因子就是：讲话者的年龄、性别、口音、讲话的单词等。在汽车相关的图片识别中，变化因子就是：汽车的位置、汽车的颜色、观察角度等。

2. 在变化因子分离的过程中，有两个问题：

    * 大多数因子仅仅影响观察到的数据的某个部分，因此需要分解这些影响数据的因子，提取我们关心的因子。

    * 从原始数据中提取某些高级的、抽象的特征可能非常困难，这使得提取这种特征几乎和解决原始问题一样难。

    如：说话者的口音只能用接近人类的、抽象的概念来表达。


3. 深度学习在学习特征时采用的解决方案是：高级特征以低级特征来表示。即：**通过组合简单的概念（`concept`）来构建复杂的概念**。

如下所示的图片识别任务中，如果直接学习从一组像素到物体的映射很困难。深度学习通过将所需的复杂映射分解成一系列嵌套的简单映射来解决该问题。每个映射由模型的不同层来描述：

    * 可见层为输入，因为它包含了能够观察到的变量。

    * 第一层隐层：描述了边(`edge`)的概念。通过比较相邻像素的亮度，则容易地识别边缘。

    * 第二层隐层：描述了角(`corner`)和轮廓(`contour`)的概念。通过识别边的集合，则容易识别角。

    * 第三层隐层：描述了特定物体整体（如：人物）的概念（物体由特定的角/等高线集合组成）。通过识别轮廓和角的特点集合，则容易识别物体整体。

    ![](../imgs/dl_intro/concepts.png)


4. 深度学习的一个经典案例是多层感知机(`multilayer perceptron:MLP`)。

一个多层感知机就是一个函数：它将一组输入值映射到输出值，而这个函数由许多更简单的函数组成。可以认为每个函数都给出了输入的一个新的`representation`。

5. 深度学习的两个观点：

    * 一个观点是：深度学习就是学习数据正确的`representaion` ，正如多层感知机所描述的。

    * 另一个观点是：深度学习让计算机学习一个多步计算程序：

        * 每一层的`representaion`被认为是在并行执行一组指令之后，计算机的存储器的状态。
        * 更深层的网络可以按顺序地执行更多的指令。
        * 序列越深的指令功能越强大，因为序列后面的指令可以参考序列前期指令的结果。

    根据这种观点：每一层的`representaion`中，并非所有的信息都对应了输入数据的特征信息，它还存储了辅助多步计算程序执行的状态信息：类似于计数器或者指针，它与输入的内容无关，但是有助于模型的组织处理过程。



### 1.3 深度

1. 深度学习的“深度”有两种度量方式。

    * 第一种度量方式为：框架中必须执行的顺序指令的数量。

    可以视为通过流程图的最长路径的长度，该流程图描述了如何根据输入来计算模型的输出。但是提供不同的函数单元，同一个模型可能具有不同的深度。

    下图给出的是`logistic regression`模型的深度。其中输出<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="3.044ex" role="img" style="vertical-align: -0.705ex;" viewbox="0 -1007.2 5279.5 1310.7" width="12.262ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" id="E2-MJMATHI-79" stroke-width="0"></path><path d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" id="E2-MJMAIN-5E" stroke-width="0"></path><path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" id="E2-MJMAIN-3D" stroke-width="0"></path><path d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z" id="E2-MJMATHI-3C3" stroke-width="0"></path><path d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" id="E2-MJMAIN-28" stroke-width="0"></path><path d="M624 444Q636 441 722 441Q797 441 800 444H805V382H741L593 11Q592 10 590 8T586 4T584 2T581 0T579 -2T575 -3T571 -3T567 -4T561 -4T553 -4H542Q525 -4 518 6T490 70Q474 110 463 137L415 257L367 137Q357 111 341 72Q320 17 313 7T289 -4H277Q259 -4 253 -2T238 11L90 382H25V444H32Q47 441 140 441Q243 441 261 444H270V382H222L310 164L382 342L366 382H303V444H310Q322 441 407 441Q508 441 523 444H531V382H506Q481 382 481 380Q482 376 529 259T577 142L674 382H617V444H624Z" id="E2-MJMAINB-77" stroke-width="0"></path><path d="M-169 694Q-169 707 -160 715T-142 723Q-127 723 -119 716T-107 698T-90 673T-53 648Q-33 637 -33 619Q-33 602 -45 595T-87 573T-144 532Q-165 513 -176 513Q-189 513 -197 522T-206 543Q-206 556 -188 574L-175 588H-347L-519 589Q-542 597 -542 618Q-542 623 -541 627T-537 635T-532 640T-527 644T-522 648L-519 649H-149Q-169 676 -169 694Z" id="E2-MJMAINB-20D7" stroke-width="0"></path><path d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z" id="E2-MJMATHI-54" stroke-width="0"></path><path d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z" id="E2-MJMAINB-78" stroke-width="0"></path><path d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" id="E2-MJMAIN-29" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="1" xlink:href="#E2-MJMATHI-79" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use x="60" xlink:href="#E2-MJMAIN-5E" xmlns:xlink="http://www.w3.org/1999/xlink" y="-13"></use><use x="837" xlink:href="#E2-MJMAIN-3D" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use x="1893" xlink:href="#E2-MJMATHI-3C3" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use x="2465" xlink:href="#E2-MJMAIN-28" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><g transform="translate(2854,0)"><use x="0" xlink:href="#E2-MJMAINB-77" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use x="686" xlink:href="#E2-MJMAINB-20D7" xmlns:xlink="http://www.w3.org/1999/xlink" y="6"></use><use transform="scale(0.707)" x="1175" xlink:href="#E2-MJMATHI-54" xmlns:xlink="http://www.w3.org/1999/xlink" y="645"></use></g><g transform="translate(4283,0)"><use x="0" xlink:href="#E2-MJMAINB-78" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use x="574" xlink:href="#E2-MJMAINB-20D7" xmlns:xlink="http://www.w3.org/1999/xlink" y="6"></use></g><use x="4890" xlink:href="#E2-MJMAIN-29" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></svg></span><script id="MathJax-Element-2" type="math/tex">\hat y=\sigma(\mathbf{\vec w}^{T}\mathbf{\vec x})</script>，<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="3.861ex" role="img" style="vertical-align: -1.639ex;" viewbox="0 -956.9 6926.9 1662.6" width="16.088ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z" id="E3-MJMATHI-3C3" stroke-width="0"></path><path d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" id="E3-MJMAIN-28" stroke-width="0"></path><path d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" id="E3-MJMATHI-7A" stroke-width="0"></path><path d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" id="E3-MJMAIN-29" stroke-width="0"></path><path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" id="E3-MJMAIN-3D" stroke-width="0"></path><path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" id="E3-MJMAIN-31" stroke-width="0"></path><path d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" id="E3-MJMAIN-2B" stroke-width="0"></path><path d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" id="E3-MJMAIN-65" stroke-width="0"></path><path d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" id="E3-MJMAIN-78" stroke-width="0"></path><path d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" id="E3-MJMAIN-70" stroke-width="0"></path><path d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" id="E3-MJMAIN-2212" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E3-MJMATHI-3C3" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use x="572" xlink:href="#E3-MJMAIN-28" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use x="961" xlink:href="#E3-MJMATHI-7A" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use x="1429" xlink:href="#E3-MJMAIN-29" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use x="2095" xlink:href="#E3-MJMAIN-3D" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><g transform="translate(2873,0)"><g transform="translate(397,0)"><rect height="60" stroke="none" width="3535" x="0" y="220"></rect><use transform="scale(0.707)" x="2249" xlink:href="#E3-MJMAIN-31" xmlns:xlink="http://www.w3.org/1999/xlink" y="571"></use><g transform="translate(60,-435)"><use transform="scale(0.707)" x="0" xlink:href="#E3-MJMAIN-31" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.707)" x="500" xlink:href="#E3-MJMAIN-2B" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><g transform="translate(903,0)"><use transform="scale(0.707)" xlink:href="#E3-MJMAIN-65" xmlns:xlink="http://www.w3.org/1999/xlink"></use><use transform="scale(0.707)" x="444" xlink:href="#E3-MJMAIN-78" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.707)" x="972" xlink:href="#E3-MJMAIN-70" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g><use transform="scale(0.707)" x="2806" xlink:href="#E3-MJMAIN-28" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.707)" x="3194" xlink:href="#E3-MJMAIN-2212" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.707)" x="3973" xlink:href="#E3-MJMATHI-7A" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use transform="scale(0.707)" x="4441" xlink:href="#E3-MJMAIN-29" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></g></g></g></svg></span><script id="MathJax-Element-3" type="math/tex">\sigma(z)=\frac{1}{1+\exp(-z)}</script>为`sigmoid`函数。

        * 左图中：将加法、乘法、`sigmoid` 函数作为基本运算单元，则模型深度为 3。
        * 右图中：将`logistic regression` 模型本身作为基本运算单元，则模型深度为 1 。

    ![](../imgs/dl_intro/depth.png)

    * 第二种度量方式为：概念`concept`图的深度。

    流程图的深度可能远远大于概念图本身，因为如果给定了复杂概念，则简单的概念可以得到更好的理解。

    如：一个面部识别应用中，如果一只眼睛在阴影中，那么`AI`最初只能看到一只眼睛。在检测到面部的存在后，`AI`可以推断出第二只眼睛很可能存在。

    此时概念图只有两层：眼睛为第一层、面部为第二层。但是此时计算概念图的流程图可能为<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="1.994ex" role="img" style="vertical-align: -0.238ex;" viewbox="0 -755.9 1100 858.4" width="2.555ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" id="E4-MJMAIN-32" stroke-width="0"></path><path d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" id="E4-MJMATHI-6E" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E4-MJMAIN-32" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use><use x="500" xlink:href="#E4-MJMATHI-6E" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></svg></span><script id="MathJax-Element-4" type="math/tex">2n</script>层，其中<span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" style="font-size: 100%; display: inline-block;" tabindex="-1"><svg focusable="false" height="1.41ex" role="img" style="vertical-align: -0.238ex;" viewbox="0 -504.6 600 607.1" width="1.394ex" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" id="E5-MJMATHI-6E" stroke-width="0"></path></defs><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use x="0" xlink:href="#E5-MJMATHI-6E" xmlns:xlink="http://www.w3.org/1999/xlink" y="0"></use></g></svg></span><script id="MathJax-Element-5" type="math/tex">n</script>是对每个概念进行修正的次数。


2. 对于模型的深度并没有一个标准的值，也没有说哪种度量方式是合适的。

    * 究竟模型的深度值为多少才能称作“深”，也没有标准答案。
    * 通常深度学习被认为是涉及大量的概念`concept`的模型的学习。


### 1.4 深度学习与 AI

1. 深度是机器学习的一种，它是一种特定类型的机器学习，通过学习将世界表示为层次化嵌套的概念。

每个概念都由一些更简单的概念定义，更抽象的特征由不那么抽象的特征来计算。

2. 下图给出了深度学习的隶属关系：深度学习 < 特征学习 < 机器学习 < 人工智能`AI` 。

![](../imgs/dl_intro/venn_diagram.png)

3. 下图给出了不同`AI`系统中，不同部分的关联。阴影方框表示计算机从数据中学习获得的部分。

    * 规则学习：硬编码知识。计算机所以无法、也不需要从数据中学习知识。
    * 经典的机器学习：人工设计特征。计算机从数据中学习到了 “特征 --> `label` ” 之间的映射。
    * 特征学习：机器从数据中自动学习到特征，然后学习到了 “特征 --> `label` ” 之间的映射。
    * 深度学习：机器从数据中自动学习到了多层特征（深层特征由浅层特征来表达），然后学习到了 “特征 --> `label` ” 之间的映射。

![](../imgs/dl_intro/AI_system.png)


## 二、历史

### 2.1 历史简介

1. 深度学习分为三个时期：

    * `1940s-1960s`：这时它被称作控制论`cybernetics` 。
    * `1980s-1990s`：这时它被称作连接机制`connectionism`。
    * `2006--`：这时被称作`deep learning` 。

深度学习在历史上的不同名字反映了不同的哲学观点。

2. 下图展示了神经网络研究的三个历史浪潮中的两个（因为第三波太近了）。

    * 第一波随着生物学习理论的发展和第一个模型的出现（如感知机神经元）。
    * 第二波用反向传播训练一层或者两层隐层神经网络。

![history](../imgs/dl_intro/history.png)

3. 第一波浪潮：早期的人工智能算法模拟生物的学习过程：对大脑的学习过程建模。

    * 此时的深度学习被称作人工神经网络`artiﬁcial neural networks:ANNs`， 深度学习模型因此被认为是受生物大脑启发的工程系统。
    * 现代的深度学习超越了神经科学的观点：它是一种多层次学习的、通用的机器学习框架，而不必是从神经科学中获取灵感。

4. 第二波浪潮：连接机制的中心思想是：大量简单的计算单元在连接时可以实现智能行为。

    * 这种观点适用于生物神经系统中的神经元，以及深度网络模型中的隐层神经元。

    * 在连接机制期间，有一些核心思想仍然影响了后续的神经网络：

        * 分布式表达`distributed representation`：系统的每个输入应该由许多特征表示，每个特征描述了输入的一个部分。

        如一个视觉识别系统可以识别：汽车、卡车、鸟，这些对象可以为红色、蓝色、绿色。

            * 表达这些输入的一种方式为：9个神经元分别表示红色卡车、红色汽车、红色鸟、绿色汽车...等等。
            * 如果使用分布式表达，则使用6个神经元：3 个神经元来表示卡车、汽车、鸟，另外3个神经元来表示红色、绿色、蓝色。


    * 反向传播算法`back-propagation`：它是当前主要的训练深度模型的算法。

    * `long short-term memory:LSTM`网络：它是一种序列模型，解决了许多自然语言处理任务。


5. 第三波浪潮从2006年开始突破。

`Geoﬀrey Hinton`给出了一种称作深度信念网络（`deep belief network`），该网络可以使用 `greedy layer-wise pre-training`策略来有效地训练。


### 2.2 目前状况

1. 深度学习的成功的关键有两个：

    * 训练集大小的增长。
    * 硬件和软件的发展。包括更快的 `CPU`、通用的`GUP`发展、更大内存、更快的网络连接、更好的软件基础。


#### 2.2.1 训练集大小

1. 早期的深度学习需要一些`trick` 才能从算法中获得良好的性能。随着训练数据量的增加，所需的`trick` 在降低。

截止2016年，一个粗略的经验法则是：

    * 在深度学习的监督学习中要想获取可接受的性能，那么每个分类集合需要大约5000个标记样本。
    * 要想匹配甚至超越人类的性能，则训练集至少包含 1000 万个标记样本。
    * 对于小于这个数量的数据集，如何获取良好的性能是个重要的研究领域。尤其关注如何使用未标记样本（通过无监督学习或者半监督学习）。

2. 下图显示了`benchmark`数据集大小随时间的变化，这种变化趋势是由于整个社会的数字化的推动。

    * 20 世纪初，统计学家使用数百或者数千的人工制作的数据来研究 。

    * 20世纪50年代到80年代，AI 专家采用较小的、合成的数据集（如低分辨率的位图的字母），从而证明神经网络能够完成特定任务。

    * 20世纪80年代到90年代，机器学习本质上更具有统计性。人们开始利用包含成千上万个例子的大数据集，如`MNIST`数据集。

    * 21世纪前十年，继续产生了同样大小的更复杂的数据集，如`CIFAR-10`数据集。

    * 2010年以来，更大的数据集包含了数十万到数千万的样本，深刻地改变了深度学习的可行性。

    ![dataset_size](../imgs/dl_intro/dataset_size.png)


3. 神经网络的表现与它的规模和数据量有关。如今在神经网络上获取更好的性能的最可靠的方法就是：训练一个更大的神经网络，以及投入更多的数据。

    * 在大规模数据集上：更大的神经网络表现得更好。它们都优于传统算法。

    * 在小规模的数据集中，各种算法的性能排名事实上不是很确定。

        * 如果没有大量的训练集，则最终效果取决于你的特征工程能力，以及在算法细节上的处理上。
        * 只有在超大规模的训练集上，神经网络才能占领统治地位。


![scale](../imgs/dl_intro/scale.png)


#### 2.2.2 计算资源

1. 神经网络最近成功的另一个关键原因是：有充足的计算资源来运行更大的模型。

2. 连接主义的主要观点是：单个神经元或者少量的神经元集合没什么用处，只有许多神经元一起工作（更大的模型）才产生智能。

更大的模型有两层含义：单个神经元的连接数量更多、模型的神经元的数量更多。

3. 生物的神经元的连接并不是非常密集。下图给出了一些动物和人的大脑中，每个神经元的连接数量。

早期人工神经元之间的连接数受到硬件能力的限制，今天神经元之间的连接数量多数出于设计考虑。目前一些人工神经网络的单个神经元的连接数量已经和猫相同。

![con_per_neurons](../imgs/dl_intro/con_per_neurons.png)

4. 在神经元总量上，早期的神经网络非常的小，直到最近才有较强的改观。

    * 自从引入了隐层以来，人工神经网络的大小大约每隔 2.4年翻一番。这种增长是由更大内存、更快的计算机（更快的`CPU`、通用`GPU`的出现、更快的网络连接、更好的软件基础）和更大的数据集来驱动的。
    * 更大的神经网络能够在更复杂的任务上实现更高的精度。
    * 按照目前的趋势，大约2050年人工神经网络将具有与人类大脑相同数量的神经元。但是生物神经元可能拥有比人工神经元更复杂的功能。

![num_neurons](../imgs/dl_intro/num_neurons.png)


#### 2.2.3 算法优化

1. 在最近几年，许多算法方面的创新也推动了神经网络的发展。这些算法主要使得神经网络运行的更快。

    * 如：激活函数从`sigmoid` 转换到 `ReLU` 函数。它使得基于梯度下降的算法运行的更快。
    * 快速计算的一个重要作用是：迭代网络的效率更高。

</body>